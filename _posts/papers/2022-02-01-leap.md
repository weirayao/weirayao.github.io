---
layout: paper
categories: papers
permalink: papers/leap
id: leap
title: "Learning Temporally Causal Latent Processes from General Temporal Data"
authors: 
  - Weiran Yao
  - Yuewen Sun
  - Changyin Sun
  - Kun Zhang
equal-contribution:
  - Weiran Yao
  - Yuewen Sun
venue: International Conference on Learning Representations
venue-shorthand: ICLR
year: 2022
url: /papers/leap
pdf: https://arxiv.org/pdf/2110.05428
code: https://github.com/weirayao/leap
talk: https://iclr.cc/virtual/2022/poster/6675
slides: https://iclr.cc/media/iclr-2022/Slides/6675.pdf
featured: false
selected: true
type: conference
figure: /images/papers/22-leap-iclr.png
feature-title: LEAP
feature-description: Tool for Recovering Causal Concepts and Relations from Videos
feature-order: 6
image: /images/featured/leap.png
bibtex: |-

    @article{yao2021learning,
        title={Learning temporally causal latent processes from general temporal data},
        author={Yao, Weiran and Sun, Yuewen and Ho, Alex and Sun, Changyin and Zhang, Kun},
        journal={arXiv preprint arXiv:2110.05428},
        year={2021}
    }
---

Our goal is to recover time-delayed latent causal variables and identify their relations from measured temporal data. Estimating causally-related latent variables from observations is particularly challenging as the latent variables are not uniquely recoverable in the most general case. In this work, we consider both a nonparametric, nonstationary setting and a parametric setting for the latent processes and propose two provable conditions under which temporally causal latent processes can be identified from their nonlinear mixtures. We propose LEAP, a theoretically-grounded framework that extends Variational AutoEncoders (VAEs) by enforcing our conditions through proper constraints in causal process prior. Experimental results on various datasets demonstrate that temporally causal latent processes are reliably identified from observed variables under different dependency structures and that our approach considerably outperforms baselines that do not properly leverage history or nonstationarity information. This demonstrates that using temporal information to learn latent processes from their invertible nonlinear mixtures in an unsupervised manner, for which we believe our work is one of the first, seems promising even without sparsity or minimality assumptions. 

---
layout: paper
categories: papers
permalink: papers/plot
id: plot
title: "PLOT: Prompt Learning with Optimal Transport for Vision-Language Models"
authors:
  - Guangyi Chen
  - Weiran Yao
  - Xiangchen Song
  - Xinyue Li
  - Yongming Rao
  - Kun Zhang
venue: International Conference on Learning Representations
venue-shorthand: ICLR
location: Kigali, Rwanda
year: 2023
url: /papers/plot
award: Oral, Top 25% Paper
pdf: https://arxiv.org/pdf/2210.01253
code: https://github.com/CHENGY12/PLOT
talk: https://iclr.cc/virtual/2023/oral/12579
slides: https://iclr.cc/virtual/2023/oral/12579
featured: false
feature-title: PLOT
feature-description: Prompt Learning with Optimal Transport for Vision-Language Models
featured-title: PLOT
selected: true
type: conference
bibtex: |-

  @inproceedings{
    chen2023plot,
    title={{PLOT}: Prompt Learning with Optimal Transport for Vision-Language Models},
    author={Guangyi Chen and Weiran Yao and Xiangchen Song and Xinyue Li and Yongming Rao and Kun Zhang},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=zqwryBoXYnh}
  }
  
---

With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at: [this URL](https://github.com/CHENGY12/PLOT).
